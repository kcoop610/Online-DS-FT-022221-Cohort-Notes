{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_42_tuning_ANNs_with_deep_NLP_v1_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QPFVXV9UC12_",
        "ClbXUn0CC13A",
        "QW6YhBrdC13I",
        "CtEsSgUpC13V",
        "lbzweQKTDPTl",
        "TtGjyt18C13V",
        "AAa41rsBC13V",
        "SDc-kq0rC13W",
        "BSBZaL8gC13W",
        "BlPs5Wd8GrNg",
        "5bOzbip5C13W",
        "95KOgmLmC13W",
        "njxrMWx-C13W",
        "DDajNP6mC13X",
        "vHpCrCW8C13X",
        "2r14qGFIVrfO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "508px",
        "left": "869.328px",
        "top": "273px",
        "width": "166.424px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjPfwXbC124"
      },
      "source": [
        "# Topic 42 - Tuning Neural Networks + Deep NLP (with Google Colab!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3dzpy4pUesE"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1F8BYCuuUI3Jcbnp70W3Uj-kSvG58uQEe?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyW7BD2KC126"
      },
      "source": [
        "- 06/09/21\n",
        "- onl01-dtsc-ft-022221"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3kX9sNhPT_p"
      },
      "source": [
        "## Colab Notebook Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy_gjQBwPWcO"
      },
      "source": [
        "- 📚: Info sections\n",
        "- 🕹: Activity sections\n",
        "    - 🎛: hyperparameters to tune\n",
        "    - 🏋️: fitting models\n",
        "    - 🤔: New Things to Potentially Try \n",
        "- Use the Table of Contents view on the left sidebar to find the relevant sections (button looks like a bulleted list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqLBiwwB6I2k"
      },
      "source": [
        "## 📚 Google Colab Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_zdpwZ6IfO"
      },
      "source": [
        "**Google Colab Quick - Notes**\n",
        " 1. **Open the sidebar!**\n",
        "    - Use `Table of Contents` to Jump between the 3 major sections.\n",
        "    - Mount your google drive via the `Files `\n",
        "    - Note: **to make a section appear in the Table of Contents, create a NEW text cell for the *header only*.** This will also let you collapse all of the cells in the section to reduce clutter.\n",
        "\n",
        "2. **Google Colab already has most common python packages.**\n",
        "    - You can pip install anything by prepending an exclamation point\n",
        "    - You can use the `IPython.display` function `clear_output` to programmatically clean up the displays from pip installation.\n",
        "    ``` python\n",
        "    !pip install fsds_100719\n",
        "    !pip install fake_useragent\n",
        "    !pip install lxml\n",
        "    from IPython.display import clear_output\n",
        "    clear_output()\n",
        "\n",
        "    %codnda install ....\n",
        "    ```\n",
        "    \n",
        "\n",
        "\n",
        "3. **Using GPUs/TPUs**\n",
        "    - `Runtime > Change Runtime Type > Hardware Acceleration`\n",
        "\n",
        "4. **Run-Before and Run-After**\n",
        "    - Go to `Runtime` and select `Run before` to run all cells up to the currently active cell\n",
        "    - Go to `Runtime` and select `Run after` to run all cells that follow the currently active cell \n",
        "\n",
        "5. **Cloud Files with Colab**\n",
        "    - **Open .csv's stored in a github repo directly with Pandas**:\n",
        "        - Go to the repo on GitHub, click on the csv file, then click on `Download` or `Raw` which will then change to show you the raw text. Copy and paste the link in your address bar (should start with www.rawgithubusercontent).\n",
        "        - In your notebook, do `df=pd.read_csv(url)` to load in the data.\n",
        "    - **Google Drive: Open sidebar > Files> click Mount Drive**\n",
        "        - or use this function (also available from file sidebar): \n",
        "        ```python \n",
        "        ## Mount Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/gdrive',force_remount=True)\n",
        "        ```\n",
        "        - Then access files by file path like usual.\n",
        "        \n",
        "    - Dropbox Files: (like images or csv)\n",
        "        - Copy and paste the share link.\n",
        "        - Change the end of the link from `dl=0`to `dl=1`\n",
        "    - Note: for some data types (like.sqlite) the only option is to store them in google drive and then mount google drive using the Files tab of the sidebar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZKUPRM0V6op"
      },
      "source": [
        "\n",
        "\n",
        "6. **Keyboard Shortcuts**\n",
        "    - A lot of the keyboard shortcuts for Colab are different.\n",
        "        - Auto-Complete is Control+Space\n",
        "        - Control+Shift+Space for docstrings.\n",
        "    - Most keyboard shortcuts can be changed\n",
        "        - go to `Tools`>`Keyboard Shortcuts`\n",
        "\n",
        "    - Some of the keyboard shortcuts are the same BUT you first have to type `Command/Cntrl + M` and THEN the keyboard shortcut. \n",
        "        - e.g. `Cmd/Cntrl+M  Y` will change a cell to a code cell\n",
        "        -  `Cmd/Cntrl+M  M` will change a code cell to a Markdown cell.\n",
        "\n",
        "\n",
        "7. **GitHub Integration**\n",
        "    \n",
        "    3. Open a notebook from github or save to github using `File > Upload Notebook` and `File> Save a copy in github`, respectively\n",
        "        - Notebooks saved to Github can optionally have a \"Open in Colab\" link inserted at the top of the notebook. \n",
        "    - You can open notebooks contained in GitHub repositories use the File menu.\n",
        "        - `File`>`Open`> `GitHub tab`\n",
        "    - You can save a copy of notebooks in GitHub repositories \n",
        "        - `File`> `Save a copy in GitHub`\n",
        "    - When you are done working for the day/want to back up the current state of your notebook:\n",
        "        - `File` > `Save and Pin Revision`\n",
        "        - This will save a revision that you could revert back to later (like a commit)\n",
        "    - **See the following example notebook from Google:**\n",
        "        - https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb \n",
        "\n",
        "    \n",
        "\n",
        "8. **You cannot easily clone an entire repository.**\n",
        "    - It is possible, but you have to do it from WITHIN a Colab notebook. \n",
        "    - **See the following resources for additional info on using Colab + GitHub**:\n",
        "        - https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228 \n",
        "\n",
        "                    \n",
        "9. **Load in images stored in a GitHub Repo for Markdown cells:**\n",
        "    - Go to Repo on GitHub.com, click on image file name.\n",
        "    - On the next page for the file, there should be a `Download` button. Click this. \n",
        "    - A new tab should open up with the raw image and the url should now read `raw.githubusercontent.com`. \n",
        "    - Copy this url, it can be used with Markdown cells using img tags. \n",
        "\n",
        "10. **Consider paying for Colab Pro if you need faster processing and more RAM**\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLEKRvAmVje3"
      },
      "source": [
        "# Original Topic 42 Notebook Continued"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81d8F9SkC127"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmqgv-bfC127"
      },
      "source": [
        "- Learn about Word Embeddings.\n",
        "    - Discuss word Embeddings and their advantages\n",
        "    - Training Word2Vec models\n",
        "    - ~~Using pretrained word embeddings~~ [Another time]\n",
        "    \n",
        "- Learn about Sequence Models and Recurrent Neural Networks\n",
        "    - LSTMs with word embeddings. \n",
        "    \n",
        "    \n",
        "- Activity: Predicting Stack Overflow post quality. \n",
        "\n",
        "- Learn about Tuning Neural Networks\n",
        "    - Discuss the different options available for tuning neural networks\n",
        "\n",
        "    - Discuss some rules of thumb for tuning Neural Networks\n",
        "\n",
        "    - Learn how to use GridSearchCV with Keras neural neural networks.\n",
        "\n",
        "    - ~~Learn how to create your own custom scorer for sklearn (and why you'd want to)~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p8v2gguC127"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0So1eHjC128"
      },
      "source": [
        "- What is an exploding Gradient?\n",
        "- Do we have the ability to set max weights?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxKd2DwOC128"
      },
      "source": [
        "# Appendix Topic: Deep Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dT0PX5NC128"
      },
      "source": [
        "## Review: NLP & Word Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSUqeitOC129"
      },
      "source": [
        "> - As a reminder, machine learning models needed text to be converted to numbers (\"vectorization\") before training the model. \n",
        "    - We used frequency counts or tf-idf values to produce numeric values for each word. \n",
        "    - We trained the models to look for the presence/absence of words to classify texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Mx9o8CC129"
      },
      "source": [
        "# 📚 Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAqsYGrKC129"
      },
      "source": [
        "- Word embeddings are vectorized words representing their **semantic meaning**.\n",
        "- They are created with an arbitrary length (typically 100 points).\n",
        "\n",
        "\n",
        "- Convert words into a vector space\n",
        "    - Each word gets its own unique vector. \n",
        "    - Vectors capture how similar various words are.\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2CTLPZ8C129"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/vectors.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuz98jHQC12-"
      },
      "source": [
        ">- Once we have word embeddings, we can actually identify related words based on meaning. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/embeddings.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvC_Z9cgC12-"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1wwTIrmC12-"
      },
      "source": [
        "- [How Embeddings are Created](https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/natural-language-processing/word2vec)\n",
        "- [Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)\n",
        "- Kaggle Tutorial:  https://www.kaggle.com/learn/embeddings\n",
        "- Google Embedding Crash Course: https://developers.google.com/machine-learning/crash-course/embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_iMlWzyC12-"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8K6uyhrC12_"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/training_data.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH6_q83YC12_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPHjj5IC12_"
      },
      "source": [
        "### Skip-Gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxJQb_yGC12_"
      },
      "source": [
        "- Train the MLP to find the best weights (context) to map word-to-word\n",
        "- But since words close to another usually contain context, we're _really_ teaching it context in those weights\n",
        "- Gut check: similar contexted words can be exchanged\n",
        "    + EX: \"A fluffy **dog** is a great pet\" <--> \"A fluffy **cat** is a great pet\"\n",
        "\n",
        "- By training a text-generation model, we wind up with a lookup table where each word has its own vector "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKEEQKk_C12_"
      },
      "source": [
        "- Resource: \n",
        "    - [skip-gram vs CBOW methods](https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPFVXV9UC12_"
      },
      "source": [
        "#### How to create an emebding:\n",
        "- Resources:\n",
        "    - [How Embeddings are Created](https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/natural-language-processing/word2vec)\n",
        "    - [Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)\n",
        "\n",
        "\n",
        "- To create a word embedding, we train a shallow neural network for a fake task. \n",
        "\n",
        "    - The fake task is to use one-hot-encoded text data to then predict the probability of seeing every other word in the corpus within the same context as the one-hot-encoded word.\n",
        "    \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/new_skip_gram_net_arch.png\">\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-using-word2vec-online-ds-ft-100719/master/images/new_word2vec_weight_matrix_lookup_table.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wM7tRU7C13A"
      },
      "source": [
        "## GloVe - Global Vectors for Word Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClbXUn0CC13A"
      },
      "source": [
        "### Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjL81kkRC13A"
      },
      "source": [
        "- Usually embeddings are hundreds of dimensions\n",
        "- Just use the word embeddings already learned from before!\n",
        "    + Unless very specific terminology, context will likely carry within language\n",
        "- Comparable to CNN transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4AYqOy3C13A"
      },
      "source": [
        "# 🕹 **Activity: Creating our own word embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQqvCZguC13B"
      },
      "source": [
        "### Data: Stack Overflow Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgsqPn9C13B"
      },
      "source": [
        "- Stack Overflow Answers: https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate\n",
        "\n",
        "\n",
        "- Kaggle Description:\n",
        "    - We collected 60,000 Stack Overflow questions from 2016-2020 and classified them into three categories:\n",
        "\n",
        "        - HQ: High-quality posts with a total of 30+ score and without a single edit.\n",
        "        - LQ_EDIT: Low-quality posts with a negative score, and multiple community edits. However, they still remain open after those changes.\n",
        "        - LQ_CLOSE: Low-quality posts that were closed by the community without a single edit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsDKF_2_DtNU"
      },
      "source": [
        "!pip install -U fsds\n",
        "from fsds.imports import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rerGHisrC13B"
      },
      "source": [
        "from tensorflow.random import set_seed\n",
        "set_seed(321)\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np \n",
        "np.random.seed(321)\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os,sys\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7oc3duC13C"
      },
      "source": [
        "## Get data from github\n",
        "url ='https://github.com/flatiron-school/Online-DS-FT-022221-Cohort-Notes/blob/master/Phase_4/topic_42_tuning_neural_networks/data/stack_overflow.csv.gz?raw=true'\n",
        "df = pd.read_csv(url,compression='gzip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIZfD5eQC13D"
      },
      "source": [
        "df['Y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU0Bw4h_C13D"
      },
      "source": [
        "### Dealing with HTML Tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PYrujmXC13D"
      },
      "source": [
        "- First, should we remove them?\n",
        "- If yes, use beautiful soup?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNrtQ_s-C13D"
      },
      "source": [
        "## Getting text example for dealing with html tags\n",
        "test_body = df.loc[4,'Body']\n",
        "test_body"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YS0QGFQC13D"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "test_soup = BeautifulSoup(test_body)\n",
        "test_soup.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "383gOOw0C13E"
      },
      "source": [
        "\n",
        "df['soups'] = df['Body'].map(lambda x:BeautifulSoup(x).text )\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HQVby_vC13E"
      },
      "source": [
        "## join together title and body. \n",
        "df['text'] = df['Title']+'; '+df['soups']\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWFsm9m-C13E"
      },
      "source": [
        "## Creating Word Embeddings with `Word2Vec`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGfSP8QPC13E"
      },
      "source": [
        "### Resources:\n",
        "\n",
        "- Two Part Word2Vec Tutorial  (linked from Canvas)\n",
        "    - [Part 1: The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "    - [Part 2: Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
        "    \n",
        "- White Paper on word2vec (downloads file):\n",
        "    - https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz0LD8DxC13F"
      },
      "source": [
        "- `sentences`: dataset to train on\n",
        "- `size`: how big of a word vector do we want\n",
        "- `window`: how many words around the target word to train with\n",
        "- `min_count`: how many times the word shows up in corpus; we don't want words that are rarely used\n",
        "- `workers`: number of threads (individual task \"workers\")\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "model.train(data, total_examples=model.corpus_count)\n",
        "\n",
        "```\n",
        "\n",
        "<!-- \n",
        "#### Word2Vec params\n",
        "\n",
        "```python\n",
        "## For initializing model\n",
        "sentences=None,\n",
        "    size=100,\n",
        "    alpha=0.025,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    max_vocab_size=None,\n",
        "    sample=0.001,\n",
        "    seed=1,\n",
        "    workers=3,\n",
        "    min_alpha=0.0001,\n",
        "    sg=0,\n",
        "    hs=0,\n",
        "    negative=5,\n",
        "    cbow_mean=1,\n",
        "    hashfxn=<built-in function hash>,\n",
        "    iter=5,\n",
        "    null_word=0,\n",
        "    trim_rule=None,\n",
        "    sorted_vocab=1,\n",
        "    batch_words=10000,\n",
        "    compute_loss=False,\n",
        "    callbacks=(),\n",
        "    \n",
        "    \n",
        "## For training \n",
        "    sentences,\n",
        "    total_examples=None,\n",
        "    total_words=None,\n",
        "    epochs=None,\n",
        "    start_alpha=None,\n",
        "    end_alpha=None,\n",
        "    word_count=0,\n",
        "    queue_factor=2,\n",
        "    report_delay=1.0,\n",
        "    compute_loss=False,\n",
        "    callbacks=(),\n",
        "``` -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrC_nqPDC13F"
      },
      "source": [
        "## NLP imports\n",
        "from nltk import word_tokenize, TweetTokenizer, regexp_tokenize\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "print(gensim.__version__)\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ooO5o5C13F"
      },
      "source": [
        "## use gensim's simple preprocess\n",
        "df['cleaned-text'] = df['text'].map(lambda x: simple_preprocess(x, deacc=True))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZphiM51C13F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv_I_1aSJIWK"
      },
      "source": [
        "### 🎛 Setting the Embedding Size, Training Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEU74kSKC13F"
      },
      "source": [
        "## set the embedding size (normally I'd do 100, but doing 50 for time)\n",
        "EMBEDDING_SIZE = 50\n",
        "# EMBEDDING_SIZE = 100\n",
        "## intiitalize the w2v odel\n",
        "w2v_model = Word2Vec(df['cleaned-text'], size=EMBEDDING_SIZE, window=5,\n",
        "                     min_count=3, workers=4, seed=321)\n",
        "\n",
        "w2v_model.corpus_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdfud0_C13G"
      },
      "source": [
        "## Train w2v model\n",
        "w2v_model.train(df['cleaned-text'],total_words=w2v_model.corpus_total_words,\n",
        "                epochs=w2v_model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQWcsyZJC13G"
      },
      "source": [
        "## w2v saves word vectors as .wv\n",
        "wv = w2v_model.wv\n",
        "wv.index2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlRAcp9KC13G"
      },
      "source": [
        "type(wv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prhssqkUC13G"
      },
      "source": [
        "## wv's vocab contains all words it learned\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W3UAlYDC13H"
      },
      "source": [
        "## wv can be used as a dictionary to extract word vectors\n",
        "wv['python']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MHAwMvCC13H"
      },
      "source": [
        "## Saving the keyed vectors as their own var\n",
        "wv.most_similar('python', topn=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-USDv6OC13H"
      },
      "source": [
        "## Can get words that are similiar or dissimilar to specific word\n",
        "# +python, -error, top 20\n",
        "wv.most_similar(positive=['python'], \n",
        "                negative=['error'],topn=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeYWoD3SC13H"
      },
      "source": [
        "##  can also do math on vectors \n",
        "# creating \"frustrating error\" from frustrating and error\n",
        "frustrating_error = wv['frustrating'] + wv['error']\n",
        "frustrating_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb9NjKB0C13I"
      },
      "source": [
        "## Can also get most_similar to word vector\n",
        "# get the most simular words to our calculated frustrating_error\n",
        "\n",
        "wv.most_similar([frustrating_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55NwNhvIC13I"
      },
      "source": [
        "# del w2v_model, df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQMNO3JJC13I"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPsURwWCC13I"
      },
      "source": [
        "## Using Embeddings in Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW6YhBrdC13I"
      },
      "source": [
        "### Embedding Layers\n",
        "You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\n",
        "\n",
        "* The embedding layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer \n",
        "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer  \n",
        "* The size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
        "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step) \n",
        "\n",
        "\n",
        "[Keras Documentation for Embedding Layers](https://keras.io/layers/embeddings/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uo3845pC13I"
      },
      "source": [
        "# 📚 Sequence Models - Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ang6xLRbC13J"
      },
      "source": [
        "- One of the main disadvantages of machine learning NLP is that the models are assesing the presence or absence of words. \n",
        "- They are not analyzing the words in the context of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKRvjT6iC13J"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-understanding-recurrent-neural-networks-online-ds-ft-100719/master/images/unrolled.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD2WtCF0C13J"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/rnn.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCIrIR1DC13J"
      },
      "source": [
        "## LSTMs & GRUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvA7wgfuC13J"
      },
      "source": [
        "- GRU (Gated Recurrent Units (GRUs)\n",
        "    - Reset Gate\n",
        "    - Update Gate\n",
        "    \n",
        "- LSTM (Long Short Term Memory Cells)\n",
        "   - Input Gate\n",
        "   - Forget Gate\n",
        "   - Output Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTk84CSlC13J"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/RNN-unrolled.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vnZDK9QC13J"
      },
      "source": [
        "Each word will have a vector of contexts: the embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZHN7KVdC13J"
      },
      "source": [
        "# 🕹 **Activity Part 2: Text Classification with Embeddings & Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFpyLKc-C13K"
      },
      "source": [
        "### Defining The Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3DsFQOpC13K"
      },
      "source": [
        "## What is the distribution of classes in our target?\n",
        "df['Y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsF18NgaC13K"
      },
      "source": [
        "## Remapping target\n",
        "target_map = {\"LQ_CLOSE\":0, \n",
        "              'LQ_EDIT': 1,\n",
        "              \"HQ\":2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQOCbCLyC13K"
      },
      "source": [
        "## map targets\n",
        "df['target'] = df['Y'].replace(target_map)\n",
        "df['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2vvGmZUC13K"
      },
      "source": [
        "#### *About that multi-classification*...\n",
        "- After a couple hours or fighting to improve the metrics for the 3-class task, I decided to create a Hot-Dog/ Not-Hot-Dog classifier. \n",
        "\n",
        "<img src=\"https://github.com/flatiron-school/Online-DS-FT-022221-Cohort-Notes/blob/master/Phase_4/topic_42_tuning_neural_networks/images/hot_dog_not_hot_dog.png?raw=1\" width=30%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRcb6X2nC13K",
        "scrolled": true
      },
      "source": [
        "## Making our hot-dog/not-dog target\n",
        "target_map_binary = {\"LQ_CLOSE\":0, \n",
        "                      'LQ_EDIT': 0,\n",
        "                      \"HQ\":1}\n",
        "df['target_binary'] = df['Y'].replace(target_map_binary)\n",
        "\n",
        "df['target_binary'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPwGl_GC13L"
      },
      "source": [
        "from tensorflow.keras import layers,optimizers,callbacks, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import text,sequence\n",
        "\n",
        "# from keras.preprocessing import text,sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5VQzKYpC13L"
      },
      "source": [
        "### 🎛 Defining X,y + train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d8NDu4RC13L"
      },
      "source": [
        "## Make X and y_t\n",
        "X = df['cleaned-text'].copy()\n",
        "\n",
        "# y_t = to_categorical(df['target'])\n",
        "y_t = to_categorical(df['target_binary'])\n",
        "\n",
        "y_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyzwnKdLC13L"
      },
      "source": [
        "X_train, X_test, y_train, y_test =train_test_split(X,y_t,test_size=0.3,\n",
        "                                                   random_state=123) \n",
        "print(X_train.shape,y_test.shape)\n",
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCszAZGIC13L"
      },
      "source": [
        "### Tokenizing with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6euG7SFC13L"
      },
      "source": [
        "## Keras has its own Tokenizer\n",
        "tokenizer = text.Tokenizer(num_words=50000)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHyoLPxpC13M"
      },
      "source": [
        "## tokenizer has assigned integer lookup value for each word\n",
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBlZKtwCC13M"
      },
      "source": [
        "# lookup the words using their integer\n",
        "tokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftqECAWvC13M"
      },
      "source": [
        "## Use tokenizer to convert texts to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "## whatn does 1 sequence look like?\n",
        "print(X_train_seq[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhqLFk6uC13M"
      },
      "source": [
        "## can lookup the words via the tokenizer's index_word \n",
        "print(' '.join([tokenizer.index_word[w] for w in X_train_seq[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd5YBgxiC13M"
      },
      "source": [
        "## We need to get all sequences as same length\n",
        "# what is the len of each sequence?\n",
        "seq_lens = [len(x) for x in X_test_seq]\n",
        "seq_lens[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KArFnKR9C13N"
      },
      "source": [
        "## what is the longest sequence?\n",
        "max(seq_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwXcOJMzC13N"
      },
      "source": [
        "## visualize distribution of lengthsw\n",
        "sns.histplot(seq_lens\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7-z1V76C13N"
      },
      "source": [
        "## What would be an approx cutoff for outliers?\n",
        "sns.boxplot(seq_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1keKp_CrJbam"
      },
      "source": [
        "### 🎛 Setting Max Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdfaLZzbC13N"
      },
      "source": [
        "## Defining a max sequence length\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_SEQUENCE_LENGTH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpCidsf1C13N"
      },
      "source": [
        "## Plot our cutoff\n",
        "## visualize distribution of lengthsw\n",
        "ax = sns.histplot(seq_lens)\n",
        "ax.axvline(MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHj2-50MC13N"
      },
      "source": [
        "## pad X_train_seq and X_test_seq\n",
        "X_train_pad = sequence.pad_sequences(X_train_seq,MAX_SEQUENCE_LENGTH)\n",
        "X_test_pad = sequence.pad_sequences(X_test_seq, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ESc7EzC13O"
      },
      "source": [
        "seq_lens = [len(x) for x in X_test_pad]\n",
        "seq_lens[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5NjUU5HC13O"
      },
      "source": [
        "#### Making Our Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcM3Rf8PC13O"
      },
      "source": [
        "## Set the max words equal to tokenizer's word index\n",
        "MAX_WORDS = len(tokenizer.word_index)\n",
        "MAX_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d1CJGJTC13O"
      },
      "source": [
        "## Save num classes for final layer\n",
        "n_classes = y_train.shape[1]\n",
        "n_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svlX49v-C13O"
      },
      "source": [
        "## 🏋️ Fitting Our First Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86rk0mL4C13O"
      },
      "source": [
        "EMBEDDING_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j9vQ7I1C13P"
      },
      "source": [
        "def make_model():\n",
        "    \"\"\"Make a neural network with a new emebdding layer, \n",
        "    an LSTM layer with 25 unit, and a final Dense layer appropriate for the task\"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(MAX_WORDS+1, EMBEDDING_SIZE))\n",
        "    model.add(layers.LSTM(25, return_sequences=False))\n",
        "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                 metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
        "    display(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWbn6VXTC13P"
      },
      "source": [
        "%%time\n",
        "## make model and fit \n",
        "model= make_model()\n",
        "history = model.fit(X_train_pad, y_train, batch_size=256, epochs=3,validation_split=0.2,\n",
        "         workers=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvjf5supC13P"
      },
      "source": [
        "### Model Evaluation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUeAteC6C13P"
      },
      "source": [
        ">- Below I broke down the larger evaluation function introduced in study group last week. \n",
        "    - I've created several helper functions to simplify the code for the evaluation function.\n",
        "    - Additionally, we can now use those smaller functions when we don't need a full model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yni1n6L_C13P"
      },
      "source": [
        "### BREAKING OUR BIG FUNCTION UP INTO HELPER FUNCTIONS\n",
        "def plot_history(history,model,figsize=(8,4)):\n",
        "    \"\"\"Takes a keras history and model and plots \n",
        "    all metrics in separate plots for each metric\"\"\"\n",
        "#     print(header,'\\t[i] MODEL HISTORY',header,sep='\\n')\n",
        "\n",
        "    ## Make a dataframe out of history\n",
        "    res_df = pd.DataFrame(history.history)#.plot()\n",
        "\n",
        "    ## Plot Losses\n",
        "    plot_kws = dict(marker='o',ls=':',lw=2,figsize=figsize)\n",
        "\n",
        "    ## Plot all metrics\n",
        "    metrics_list = model.metrics_names\n",
        "\n",
        "    for metric in metrics_list:\n",
        "        ax = res_df[[col for col in res_df.columns if metric in col]].plot(**plot_kws)\n",
        "        ax.set(xlabel='Epoch',ylabel=metric,title=metric)\n",
        "        ax.grid()\n",
        "        ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "## testing function\n",
        "plot_history(history, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGtx4xxVC13Q"
      },
      "source": [
        "def evaluate_scores(model,X_train,y_train,label='Training',verbose=0):\n",
        "    \"\"\"Evaluates a keras model and prints the scores using the provided label.\"\"\"\n",
        "    train_scores  = model.evaluate(X_train,y_train,verbose=verbose)#score()\n",
        "    for i,metric in enumerate(model.metrics_names):\n",
        "        print(f\"\\t{label} {metric}: {train_scores[i]:.3f}\")\n",
        "\n",
        "evaluate_scores(model, X_train_pad,y_train,verbose=1)\n",
        "evaluate_scores(model, X_test_pad,y_test,verbose=1,label='Test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR1Y3XFoC13Q"
      },
      "source": [
        "  def classification_report_cm(model, X_train,y_train,label='TRAINING DATA',\n",
        "                            cm_figsize=(6,6),normalize='true',cmap='Greens'):\n",
        "    \"\"\"Gets predictions from a Keras neural network and get \n",
        "    classification report and confusion matrix.\"\"\"\n",
        "    ## Print report header, get preds, get class report, and conf matrix\n",
        "    header =  '==='*24\n",
        "    print(header,f\"\\t[i] CLASSIFICATION REPORT - {label}\",header,sep='\\n')\n",
        "    print()\n",
        "    \n",
        "    ## Get predictions\n",
        "    y_hat_train = model.predict(X_train)\n",
        "    \n",
        "    ## convert to 1D targets\n",
        "    y_train_class =y_train.argmax(axis=1)\n",
        "    y_hat_train_class = y_hat_train.argmax(axis=1)\n",
        "    \n",
        "    \n",
        "    ## Get classification report \n",
        "    print(metrics.classification_report(y_train_class,y_hat_train_class))\n",
        "    print()\n",
        "    \n",
        "    \n",
        "    ## Plot the confusion Matrix\n",
        "    cm = metrics.confusion_matrix(y_train_class, y_hat_train_class,\n",
        "                                  normalize=normalize)\n",
        "    \n",
        "    fig,ax = plt.subplots(figsize=cm_figsize)\n",
        "    sns.heatmap(cm, cmap=cmap, annot=True,square=True,ax=ax)\n",
        "    ax.set(ylabel='True Class',xlabel='Predicted Class',\n",
        "           title='Confusion Matrix - Training Data')    \n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "classification_report_cm(model,X_test_pad, y_test, label='TEST DATA')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjLV7fYuC13Q"
      },
      "source": [
        "def evaluate_network(model, X_test, y_test, history=None, \n",
        "                        X_train = None, y_train = None,\n",
        "                        history_figsize = (8,4), cm_figsize=(8,8),\n",
        "                        cmap='Greens', normalize='true'):\n",
        "    \"\"\"Gets predictions and evaluates a classification model using\n",
        "    sklearn.\n",
        "\n",
        "    Args:\n",
        "        model (classifier): a fit keras classification model.\n",
        "        X_test (tensor/array): X data\n",
        "        y_test (tensor/array): y data\n",
        "        history (History object): model history from .fit\n",
        "        X_train (tensor/array): If provided, compare model.score \n",
        "                                for train and test. Defaults to None.\n",
        "        y_train (Series or Array, optional): If provided, compare model.score \n",
        "                                for train and test. Defaults to None.\n",
        "                                \n",
        "        history_figsize (tuple): figsize for each metric's history plot.\n",
        "        cm_figsize (tuple): figsize for confusion matrix plot\n",
        "      \n",
        "        cmap (str, optional): Colormap for confusion matrix. Defaults to 'Greens'.\n",
        "        normalize (str, optional): normalize argument for plot_confusion_matrix. \n",
        "                                    Defaults to 'true'.  \n",
        "    \"\"\"\n",
        "    \n",
        "    header =  '==='*24\n",
        "    \n",
        "    ## First, Plot History, if provided.\n",
        "    if history is not None:\n",
        "        print(header,'\\t[i] MODEL HISTORY',header,sep='\\n')\n",
        "        plot_history(history,model,figsize=history_figsize)\n",
        "        \n",
        "        \n",
        "    ## Evaluate Network for loss/acc scores\n",
        "    print(header,\"\\t[i] EVALUATING MODEL\",header,sep='\\n')\n",
        "    print()\n",
        "    if X_train is not None:\n",
        "        try:\n",
        "            evaluate_scores(model,X_train,y_train,label='Training')\n",
        "            print()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error evaluating for accuracy for training data:\")\n",
        "            print(e)\n",
        "        \n",
        "\n",
        "    ## Evaluate test data\n",
        "    evaluate_scores(model,X_test,y_test,label='Test')\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \n",
        "    ## Report for training data\n",
        "    if X_train is not None:\n",
        "        classification_report_cm(model, X_train, y_train, cmap=cmap,\n",
        "                                 normalize=normalize,\n",
        "                                 label='TRAINING DATA',cm_figsize=cm_figsize)       \n",
        "        print('\\n'*2)\n",
        "    ## Report for test data\n",
        "    classification_report_cm(model,X_test,y_test, cmap=cmap,\n",
        "                             normalize=normalize,\n",
        "                             label='TEST DATA',cm_figsize=cm_figsize)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXWibrQ9C13R",
        "scrolled": false
      },
      "source": [
        "## make,fit model and evlaute\n",
        "model = make_model()\n",
        "history = model.fit(X_train_pad, y_train, epochs=3,\n",
        "                    batch_size=128, validation_split=0.2,\n",
        "                   workers=-1)\n",
        "evaluate_network(model,X_test_pad,y_test,history,\n",
        "                X_train = X_train_pad,y_train=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s_jSAmiC13R"
      },
      "source": [
        ">- **Q: Whats one thing we haven't addressed, as part of classification-modeling workflow?**\n",
        "    - Dealing with class imbalance/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAthFQjAC13R"
      },
      "source": [
        "### Compute Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDv_j3cHC13R"
      },
      "source": [
        "## check class balance\n",
        "y_tr_classes = pd.Series(y_train.argmax(axis=1))\n",
        "y_tr_classes.value_counts(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4nE8LqAC13R"
      },
      "source": [
        "> Neural networks accept class weights, but cannot calculate them like sklearn models that accept `class_weight=\"balanced\"`.\n",
        "    - We can use sklearn's function to calculate the class weights to use in our neural netowork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErfPNB4oC13R"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "## Get the array of weights for each unique class\n",
        "weights= compute_class_weight(\n",
        "           'balanced',\n",
        "            np.unique(y_tr_classes),\n",
        "            y_tr_classes)\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oqE4ZG7C13S",
        "scrolled": true
      },
      "source": [
        "## Turn the weights into a dict with the class name as the key\n",
        "weights_dict = dict(zip( np.unique(y_tr_classes),weights))\n",
        "weights_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myF2_D0EC13S",
        "scrolled": false
      },
      "source": [
        "## make,fit model and evlaute\n",
        "model = make_model()\n",
        "history = model.fit(X_train_pad, y_train, epochs=3,\n",
        "                    batch_size=128, validation_split=0.2,\n",
        "                   workers=-1,class_weight=weights_dict)\n",
        "evaluate_network(model,X_test_pad,y_test,history,\n",
        "                X_train = X_train_pad,y_train=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVwEmVC_C13T"
      },
      "source": [
        "## 🏋️ Using our Previously Trained Word2Vec Embeddings in an Embedding Layer\n",
        "- https://ppasumarthi-69210.medium.com/word-embeddings-in-keras-be6bb3092831"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1SAKEA6C13U"
      },
      "source": [
        "## Saving the total number of words as vocab size\n",
        "vocab_size = len(tokenizer.index_word)\n",
        "\n",
        "## Doubel check current embedding size and vocab size\n",
        "vocab_size, EMBEDDING_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY9FqSSwC13U"
      },
      "source": [
        "### make a metrix of embedding weights\n",
        "embedding_matrix = np.zeros((vocab_size+1, EMBEDDING_SIZE))\n",
        "\n",
        "## for each item in the word index\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\n",
        "    ## if word in w2vec model, fill in the embedding matrix\n",
        "     if word in wv:\n",
        "        embedding_vector = wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1MUrZDZC13U"
      },
      "source": [
        "## Make a keras embedding layer with emebdding_matrix\n",
        "embedding_layer = layers.Embedding(vocab_size+1,EMBEDDING_SIZE,\n",
        "                                  weights=[embedding_matrix],\n",
        "                                  input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                  trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvtM4YJWC13U"
      },
      "source": [
        "## update our make_model_w2v func wth embedding layer\n",
        "def make_w2v_model():\n",
        "    \"\"\"Make a neural network with a new emebdding layer, \n",
        "    an LSTM layer with 25 unit, and a final Dense layer appropriate for the task\"\"\"\n",
        "    model = models.Sequential()\n",
        "    embedding_layer = layers.Embedding(vocab_size+1,EMBEDDING_SIZE,\n",
        "                                  weights=[embedding_matrix],\n",
        "                                  input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                  trainable=False)\n",
        "    model.add(embedding_layer)\n",
        "#     model.add(layers.Embedding(MAX_WORDS+1, EMBEDDING_SIZE))\n",
        "    model.add(layers.LSTM(25, return_sequences=False))\n",
        "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                 metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
        "    display(model.summary())\n",
        "    return model\n",
        "\n",
        "## make,fit model and evlaute\n",
        "model = make_w2v_model()\n",
        "history = model.fit(X_train_pad, y_train, epochs=3,\n",
        "                    batch_size=128, validation_split=0.2,\n",
        "                   workers=-1,class_weight=weights_dict)\n",
        "evaluate_network(model,X_test_pad,y_test,history,\n",
        "                X_train = X_train_pad,y_train=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64h9ocgOC13U"
      },
      "source": [
        "# 📚 Overview  - Neural Network Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBJCBTdBC13U"
      },
      "source": [
        "## Helpful Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r19ehdwMC13U"
      },
      "source": [
        "- [Medium: Simple Guide to Hyperparameter Tuning in Neural Networks](https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594)\n",
        "- [Medium: A guide to an efficient way to build neural network architectures- Part I:](https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b)\n",
        "- [Medium: Optimizers for Neural Networks](https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtEsSgUpC13V"
      },
      "source": [
        "## Rules of Thumb for Training Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0_jp6KC13V"
      },
      "source": [
        "- **Always use a train-test-validation split.**\n",
        "    - **Train-test-val splits:**\n",
        "        - Training set: for training the algorithm\n",
        "        - Validation set: used during training\n",
        "        - Testing set: after choosing the final model, use the test set for an unbiased estimate of performance.\n",
        "    - **Set sizes:**\n",
        "        - With big data, your val and test sets don't necessarily need to be 20-30% of all the data. \n",
        "        - You can choose test and hold-out sets that are of size 1-5%. \n",
        "            - eg. 96% train, 2% hold-out, 2% test set.\n",
        "            \n",
        "            \n",
        "- Consider using a `np.random.seed` for reproducibility/comparing models\n",
        "\n",
        "\n",
        "- **Use cross validation of some sort to compare Networks**\n",
        "\n",
        "\n",
        "- Normalize/Standardize features\n",
        "    \n",
        "    \n",
        "- **Add EarlyStopping and ModelCheckpoint [callbacks](https://keras.io/api/callbacks/)**\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzweQKTDPTl"
      },
      "source": [
        "#### Dealing with Bias/Variance\n",
        "\n",
        "- Balancing Bias/Variance:\n",
        "    - High Bias models are **underfit**\n",
        "    - High Variance models are **overfit**\n",
        "\n",
        "\n",
        "\n",
        "- **Rules of thumb re: bias/variance trade-off:**\n",
        "\n",
        "| High Bias? (training performance) | high variance? (validation performance)  |\n",
        "|---------------|-------------|\n",
        "| Use a bigger network|    More data     |\n",
        "| Train longer | Regularization   |\n",
        "| Look for other existing NN architextures |Look for other existing NN architextures |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtGjyt18C13V"
      },
      "source": [
        "## Rules of Thumb - Hyperparameters to Tune \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGoRXyZSC13V"
      },
      "source": [
        "- This section is partially based on the blog post below. \n",
        "- However, I ordered the steps with my recommended order of importance/what-to-tune-first\n",
        "- [Blog Post](https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAa41rsBC13V"
      },
      "source": [
        "### Hyperparameters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wupQi2j1C13V"
      },
      "source": [
        "- Note: outline below is meant for Dense layers but will also generally be true for other layer types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQYCqjbLC13W"
      },
      "source": [
        "1. Number of layers (depends on the size of training data)\n",
        "\n",
        "\n",
        "2. Number of neurons(depends on the size of training data)\n",
        "\n",
        "\n",
        "3. Activation functions\n",
        "    - Popular choices:\n",
        "        - relu / leaky-relu\n",
        "        - sigmoid / tanh (for shallow networks)\n",
        "        \n",
        "        \n",
        "4. Optimizer:\n",
        "    - Popular choices:\n",
        "        - SGD (works well for shallow but gets stuck in local minima/saddle-points - if so use RMSProp)\n",
        "        - RMSProp\n",
        "        - Adam (general favorite)\n",
        "        \n",
        "        \n",
        "5. Learning Rate\n",
        "    - Try in powers of 10 (0.001,0.01,.1,1.0)\n",
        "    - Which optimizer changes which l.r. is best (but try the others too).\n",
        "        - SGD: 0.1\n",
        "        - Adam: 0.001/0.01\n",
        "    - Can use the `decay` parameter to reduce learning (though it is better to use adaptive optimizer than to adjust this)/.\n",
        "\n",
        "7. Batch Size\n",
        "    - Finding the \"right\" size is important\n",
        "        - Too small = weights update too quickly and convergence is difficult\n",
        "        - Too large = weights update too slowly (plus PC RAM issues)\n",
        "    - Try batch sizes that are powers of 2 (for memory optimization)\n",
        "    - Larger is better than smaller.\n",
        "    \n",
        "    \n",
        "8. Number of Epochs:\n",
        "    - Important parameter to tune\n",
        "    - Use EarlyStopping callback to prevent overfitting\n",
        "    \n",
        "\n",
        "9. Adding Dropout\n",
        "    - Usually use dropout rate of 0.2 to 0.5\n",
        "    \n",
        "    \n",
        "\n",
        "10. Adding regularization (L1,L2)\n",
        "    - Use when the model continues to over-fit even after adding Dropout\n",
        "    \n",
        "    \n",
        "6. Initialization\n",
        "    - Not as important as defaults (glorot-uniform) work well, but:\n",
        "        - Use He-normal/uniform initialization when using ReLu\n",
        "        - Use Glorot-normal/uniform when using Sigmoid\n",
        "    - Avoid using all zeros or any constant for all neurons\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDc-kq0rC13W"
      },
      "source": [
        "### Easy-to-Add options to fight overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSBZaL8gC13W"
      },
      "source": [
        "#### Dropout \n",
        "<img src=\"https://raw.githubusercontent.com/flatiron-school/Online-DS-FT-022221-Cohort-Notes/master/Phase_4/topic_40-41_neural_networks/CL%20Repos/ds-neural_network_architecture-video/img/drop_out.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlPs5Wd8GrNg"
      },
      "source": [
        "#### Early Stopping\n",
        "\n",
        "- Monitor performance for decrease or plateau in performance, terminate process when given criteria is reached.\n",
        "\n",
        "- **In Keras:**\n",
        "    - Can be applied using the [callbacks function](https://keras.io/callbacks/)\n",
        "```python    \n",
        "from keras.callbacks import EarlyStopping\n",
        "EarlyStopping(monitor='val_err', patience=5)\n",
        "```\n",
        "    - 'Monitor' denotes quanitity to check\n",
        "    - 'val_err' denotes validation error\n",
        "    - 'pateience' denotes # of epochs without improvement before stopping.\n",
        "        - Be careful, as sometimes models _will_ continue to improve after a stagnant period\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bOzbip5C13W"
      },
      "source": [
        "### Hyperparameter Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95KOgmLmC13W"
      },
      "source": [
        "#### Kernel Initialization\n",
        "- Kernel Initializers\n",
        "```python\n",
        "# define the grid search parameters\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
        "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']```\n",
        "\n",
        "\n",
        "#### Loss Functions\n",
        "- MSE (regression)\n",
        "- categorical cross-entropy (classification with 2D labels )\n",
        "    - sparse categorical cross entropy (classification with 1D labels)\n",
        "- binary cross-entropy (classification)\n",
        "    - 2 categories\n",
        "- **can also uses custom scoring functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njxrMWx-C13W"
      },
      "source": [
        "## Using Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDajNP6mC13X"
      },
      "source": [
        "### L1 & L2 Regularlization\n",
        "- These methods of regularizaiton do so by penalizing coefficients(regression) or weights(neural networks),\n",
        "    - L1 & L2 exist in regression models as well. There, L1='Lasso Regressions' , L2='Ridge regression'\n",
        "    \n",
        "<!--     \n",
        "\n",
        "- **L1 & L2 regularization add a term to the cost function.**\n",
        "\n",
        "$$Cost function = Loss (say, binary cross entropy) + Regularization term$$\n",
        "\n",
        "$$ J (w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}\\sum^L_{l=1}||w^{[l]}||^2$$\n",
        "- where $\\lambda$ is the regularization parameter. \n",
        "\n",
        "- **The difference between  L1 vs L2 is that L1 is just the sum of the weights whereas L2 is the sum of the _square_of the weights.**  \n",
        " -->\n",
        "\n",
        "<br><br>\n",
        "- **L1 Regularization:**\n",
        "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||$$\n",
        "    - Uses the absolute value of weights and may reduce the weights down to 0. \n",
        "    \n",
        "        \n",
        "- **L2 Regularization:**:\n",
        "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||^2$$\n",
        "    - Also known as weight decay, as it forces weights to decay towards zero, but never exactly 0.. \n",
        "\n",
        "#### L1/L2 Regularization\n",
        "\n",
        "- **CHOOSING L1 OR L2:**\n",
        "    - L1 is very useful when trying to compress a model. (since weights can decreae to 0)\n",
        "    - L2 is generally preferred otherwise.\n",
        "    \n",
        "    \n",
        "- **USING L1/L2 IN KERAS:**\n",
        "    - Add a kernel_regulaizer to a  layer.\n",
        "```python \n",
        "from keras import regularizers\n",
        "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01))\n",
        "```\n",
        "    - here 0.01 = $\\lambda$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCIcE6gZC13X"
      },
      "source": [
        "# 🕹 Activity Part 2: Tuning Our Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHpCrCW8C13X"
      },
      "source": [
        "## Adding Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r14qGFIVrfO"
      },
      "source": [
        "### Keras Callbacks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtTKy31agUb6"
      },
      "source": [
        "- [Official Callback documentation](https://keras.io/callbacks/)\n",
        "- CallBacks You'll Definitely Want to Use\n",
        " - `tensorflow.keras.callbacks.EarlyStopping`[ALWAYS!]\n",
        "  - `tensorflow.keras.callbacks.ModelCheckpoint` [Always, if on Colab]\n",
        "\n",
        "- Callbacks worth further exploration\n",
        " - `tensorflow.keras.callbacks.callbacks.LearningRateScheduler`\n",
        "     - May be outdated in tf 2.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QT0_OfvC13X"
      },
      "source": [
        "## import callbacks\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5AoZLEzC13X"
      },
      "source": [
        "## Make folder for models\n",
        "model_folder = './models/'\n",
        "os.makedirs(model_folder,exist_ok=True)\n",
        "os.listdir(model_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi66iqDLC13Y"
      },
      "source": [
        "## make checkpoints - early stopping and modelcheckpoint\n",
        "early_stop = EarlyStopping(monitor='val_accuracy',patience=2,verbose=1,\n",
        "                          restore_best_weights=False)\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_folder,verbose=0,save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFySVB1IC13Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R16oZqreC13Y"
      },
      "source": [
        "### You can use {} to insert values in your checkpoint names\n",
        "# filepath=folder+\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "# checkpoint = ModelCheckpoint(filepath,verbose=1,save_best_only=True,\n",
        "#                             save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pceVY7xIC13Y",
        "scrolled": false
      },
      "source": [
        "## paste in our prior model function and fitting/eval code, but add callbacks\n",
        "\n",
        "## update our make_model_w2v func wth embedding layer\n",
        "def make_w2v_model():\n",
        "    \"\"\"Make a neural network with a new emebdding layer, \n",
        "    an LSTM layer with 25 unit, and a final Dense layer appropriate for the task\"\"\"\n",
        "    model = models.Sequential()\n",
        "    embedding_layer = layers.Embedding(vocab_size+1,EMBEDDING_SIZE,\n",
        "                                  weights=[embedding_matrix],\n",
        "                                  input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                  trainable=False)\n",
        "    model.add(embedding_layer)\n",
        "    model.add(layers.LSTM(25, return_sequences=False))\n",
        "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                 metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
        "    display(model.summary())\n",
        "    return model\n",
        "\n",
        "## make,fit model and evlaute\n",
        "model = make_w2v_model()\n",
        "history = model.fit(X_train_pad, y_train, epochs=5,\n",
        "                    batch_size=128, validation_split=0.2,\n",
        "                   workers=-1,class_weight=weights_dict,callbacks=[early_stop])\n",
        "evaluate_network(model,X_test_pad,y_test,history,\n",
        "                X_train = X_train_pad,y_train=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPYYUfNnC13Y"
      },
      "source": [
        "# 📚 Gridsearching with Keras & Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ts-O_4C13Y"
      },
      "source": [
        "## HyperParameter Tuning with GridSearchCV & Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW1lJ_O9C13Y"
      },
      "source": [
        "Original Source: https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/\n",
        "<br><br>\n",
        "\n",
        "- To use `GridSearchCV` or other similar functions in scikit-learn with a Keras neural network, we need to wrap our keras model in `keras.wrappers.scikit_learn`'s `KerasClassifier` and `KerasRegressor`.\n",
        "\n",
        "1. To do this, we need to write a build function(`build_fn`) that creates our model such as `create_model`.\n",
        "    - This function must accept whatever parameters you wish to tune. \n",
        "    - It also must have a default argument for each parameter.\n",
        "    - This function must Return the model (and only the model)\n",
        "    \n",
        "<!-- \n",
        "```python\n",
        "\n",
        "## Define the build function\n",
        "def create_model(n_units=(50,25,7), activation='relu',final_activation='softmax',\n",
        "                optimizer='adam'):\n",
        "    \n",
        "    ## Pro tip:save the local variables now so you can print out the parameters used to create the model.\n",
        "    params_used = locals()\n",
        "    print('Parameters for model:\\n',params_used)\n",
        "    \n",
        "   \n",
        "    from keras.models import Sequential\n",
        "    from keras import layers\n",
        "    \n",
        "    model=Sequential()\n",
        "    model.add(layers.Dense(n_units[0], activation=activation, input_shape=(2000,)))\n",
        "    model.add(layers.Dense(n_units[1], activation=activation))\n",
        "    model.add(layers.Dense(n_units[2], activation=final_activation))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    display(model.summary())\n",
        "    return model \n",
        "```     -->\n",
        "\n",
        "2. We then create out model using the Keras wrapper:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "neural_network =  KerasClassifier(build_fn=create_model,verbose=1)\n",
        "```\n",
        "\n",
        "3. Now, set up the hyperparameter space for grid search. (Remember, your `create_model` function must accept the parameter you want to tune)\n",
        "\n",
        "```python\n",
        "params_to_test = {'n_units':[(50,25,7),(100,50,7)],\n",
        "                  'optimizer':['adam','rmsprop','adadelta'],\n",
        "                  'activation':['linear','relu','tanh'],\n",
        "                  'final_activation':['softmax']}\n",
        "```\n",
        "\n",
        "4. Now instantiate your GridSearch function\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(estimator=neural_network,param_grid=params_to_test)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "best_params = grid_result.best_params_\n",
        "```\n",
        "5. And thats it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1JSxAyJ9DX"
      },
      "source": [
        "### 🎛 Defining Build Function and Params Grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFwZmhmAC13Z"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cDqTZ7KC13Z"
      },
      "source": [
        "# make a new make_tune_model function to tune the # of units\n",
        "# and if embeddings are trainable\n",
        "def make_tune_model(n_units_lstm=25, trainable=False):\n",
        "    \"\"\"Make a neural network with a new emebdding layer, \n",
        "    an LSTM layer with 25 unit, and a final Dense layer appropriate for the task\"\"\"\n",
        "    model = models.Sequential()\n",
        "    embedding_layer = layers.Embedding(vocab_size+1,EMBEDDING_SIZE,\n",
        "                                  weights=[embedding_matrix],\n",
        "                                  input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                  trainable=trainable)\n",
        "    model.add(embedding_layer)\n",
        "    model.add(layers.LSTM(n_units_lstm, return_sequences=False))\n",
        "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                 metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
        "    display(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy67mcJVC13Z"
      },
      "source": [
        "# ## make a model with sklearn wrapper\n",
        "# wrapped_model = KerasClassifier(make_tune_model)\n",
        "# wrapped_model.fit(X_train_pad,y_train, \n",
        "#                   epochs=3, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tqf6snC13Z"
      },
      "source": [
        "## make anew early stopping with shorter patience, do not restore best weights\n",
        "early_stop = EarlyStopping(monitor='val_accuracy',patience=0,verbose=1,\n",
        "                          restore_best_weights=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vosPKduBC13Z"
      },
      "source": [
        "## Set up params grid for 25,509 units and trianable true/false\n",
        "\n",
        "## Set up params grid for 25,509 units and trianable true/false\n",
        "params = {'n_units_lstm':[25,50],\n",
        "         'trainable':[True,False]}\n",
        "\n",
        "\n",
        "## Make and fit grid, check best params\n",
        "grid = GridSearchCV(KerasClassifier(make_tune_model), \n",
        "                    params,cv=2,n_jobs=-1,verbose=1)\n",
        "\n",
        "grid.fit(X_train_pad,y_train, epochs=3, \n",
        "         callbacks=[early_stop],\n",
        "         validation_split=0.2)\n",
        "\n",
        "grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZADA5U-IC13Z"
      },
      "source": [
        "## whats the best score?\n",
        "best_ann = grid.best_estimator_\n",
        "history = best_ann.fit(X_train_pad,y_train, epochs=3, \n",
        "         callbacks=[early_stop],\n",
        "         validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ov1CdhDC13a"
      },
      "source": [
        "best_ann.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKVF4usIC13a",
        "scrolled": false
      },
      "source": [
        "evaluate_network(best_ann.model,X_test_pad,y_test,history=history,\n",
        "                 X_train=X_train_pad,y_train = y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxkJIPScC13a"
      },
      "source": [
        "raise Exception('The following cells are not guaranteed to run!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdyqrl04C13a"
      },
      "source": [
        "# 🗄 APPENDIX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592E5aLgC13a"
      },
      "source": [
        "## 🤔 Tensorboard Callback\n",
        "- https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyn3V_jIC13a"
      },
      "source": [
        "> Add tensorboard callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMnTICHMC13a"
      },
      "source": [
        "%load_ext tensorboard\n",
        "logdir = './logs/'\n",
        "os.makedirs(logdir,exist_ok=True)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xNbs4-sC13b",
        "scrolled": false
      },
      "source": [
        "## Test using function to train and evaluate\n",
        "model = make_model()\n",
        "history = model.fit(X_train_pad, y_train, epochs=3,\n",
        "                    batch_size=64, validation_split=0.2,\n",
        "                   workers=-1,callbacks=[tensorboard_callback])\n",
        "evaluate_network(model,X_test_pad,y_test,history,\n",
        "                X_train = X_train_pad,y_train=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Q2l09zC13b"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHYsOpAzC13b"
      },
      "source": [
        "## 🤔 Using Pre-Trained Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BZ35Xm-UG26"
      },
      "source": [
        "### On Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRyw0TQQRa44"
      },
      "source": [
        ">- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) - official website with documentation. \n",
        "    - There are several different pretrained vectors available. We will use the default/starter set of vectors, but you could select a different link to download an alternative option\n",
        "        - Recommended link: http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        ">- [Tutorial on using Glove with Colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/pretrained_word_embeddings.ipynb#scrollTo=b_H-URXmROE6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLMTquK-RaQ_"
      },
      "source": [
        "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "!wget {glove_url}\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFJ8OXbnUwg_"
      },
      "source": [
        "## Delete the zip file\n",
        "!rm glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3EUcqZlUUiX"
      },
      "source": [
        "# path_to_glove_file = os.path.join(\n",
        "#     os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",
        "# )\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTtNJYMOVIy6"
      },
      "source": [
        "### Local Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbi1QUEPC13c"
      },
      "source": [
        "# import os\n",
        "# folder = '/Users/jamesirving/Datasets/'#glove.twitter.27B/'\n",
        "# # print(os.listdir(folder))\n",
        "# glove_file = folder+'glove.6B/glove.6B.50d.txt'#'glove.twitter.27B.50d.txt'\n",
        "# glove_twitter_file = folder+'glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
        "# print(glove_file)\n",
        "# print(glove_twitter_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy3-MuAOC13c"
      },
      "source": [
        "#### Keeping only the vectors needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxRrCYtkC13c"
      },
      "source": [
        "# ## This line of code for getting all words bugs me\n",
        "# total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
        "# len(total_vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTtKDr1DC13c"
      },
      "source": [
        "# glove = {}\n",
        "# with open(glove_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
        "#     for line in f:\n",
        "#         parts = line.split()\n",
        "#         word = parts[0].decode('utf-8')\n",
        "#         if word in total_vocabulary:\n",
        "#             vector = np.array(parts[1:], dtype=np.float32)\n",
        "#             glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-E24QRqC13c"
      },
      "source": [
        "# glove['republican']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orO5_GS7C13c"
      },
      "source": [
        "### Converting Glove to Word2Vec format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbDW8dKjC13c"
      },
      "source": [
        "- Getting glove into w2vec format:\n",
        "    - https://radimrehurek.com/gensim/scripts/glove2word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYvLJqTSC13c"
      },
      "source": [
        "glove_folder = folder+'glove.twitter.27B'\n",
        "os.listdir(glove_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAlKFKg5C13d"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file = datapath(glove_twitter_file)\n",
        "tmp_file = get_tmpfile(glove_folder+'glove_to_w2vec.txt')\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "model_glove = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSu0KZchC13d"
      },
      "source": [
        "model_glove.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET327KobC13b"
      },
      "source": [
        "## LSTM vs GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1-pNz9sC13b"
      },
      "source": [
        "## GRU Model\n",
        "from keras import models, layers, optimizers, regularizers\n",
        "modelG = models.Sequential()\n",
        "\n",
        "## Get and add embedding_layer\n",
        "# embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
        "modelG.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
        "\n",
        "# modelG.add(layers.SpatialDropout1D(0.5))\n",
        "# modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)))\n",
        "modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2)))\n",
        "modelG.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "modelG.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
        "modelG.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uiI6-MyC13b"
      },
      "source": [
        "\n",
        "history = modelG.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "y_hat_test = modelG.predict_classes(X_test)\n",
        "kg.evaluate_model(y_test,y_hat_test,history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfM0KVjwC13d"
      },
      "source": [
        "## Using Embeddings in Classification Models - sci-kit learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM0FM3VYC13d"
      },
      "source": [
        "- Embeddings can be used in Artificial Neural Networks as an input Embedding Layer\n",
        "- Embeddings can be used in sci-kit learn models by taking the mean vector of a text/document and using the mean vector as the input into the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhP1nVB5C13d"
      },
      "source": [
        "### Creating Mean Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1xGAjdKC13d"
      },
      "source": [
        "## This line of code for getting all words bugs me\n",
        "total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
        "len(total_vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2geP4BBRC13d"
      },
      "source": [
        "glove = {}\n",
        "with open(glove_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
        "    for line in f:\n",
        "        parts = line.split()\n",
        "        word = parts[0].decode('utf-8')\n",
        "        if word in total_vocabulary:\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8P1MYzXC13d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "\n",
        "y = pd.get_dummies(df['is_trump'],drop_first=True).values\n",
        "X = df['text'].str.lower().map(word_tokenize)\n",
        "\n",
        "X_idx = list(range(len(X)))\n",
        "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
        "\n",
        "X[train_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIo7gEP3C13e"
      },
      "source": [
        "def train_test_split_idx(X, y, train_idx,test_idx):\n",
        "    # try count vectorized first\n",
        "    X_train = X[train_idx].copy()\n",
        "    y_train = y[train_idx].copy()\n",
        "    X_test = X[train_idx].copy()\n",
        "    y_test = y[train_idx].copy()\n",
        "    return X_train, X_test,y_train, y_test\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split_idx(X,y,train_idx,test_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa7SLLGTC13e"
      },
      "source": [
        "# df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
        "# data = df['combined_text'].map(word_tokenize).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt0bD2fQC13e"
      },
      "source": [
        "class W2vVectorizer(object):\n",
        "    \n",
        "    def __init__(self, w2v):\n",
        "        # Takes in a dictionary of words and vectors as input\n",
        "        self.w2v = w2v\n",
        "        if len(w2v) == 0:\n",
        "            self.dimensions = 0\n",
        "        else:\n",
        "            self.dimensions = len(w2v[next(iter(glove))])\n",
        "    \n",
        "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
        "    # it can't be used in a scikit-learn pipeline  \n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "            \n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
        "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}