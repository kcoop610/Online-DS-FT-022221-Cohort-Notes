{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 23: Cost Functions and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- onl01-dtsc-ft-022221\n",
    "- 04/30/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Learn about derivatives and their rules, and how we can use derivatives to find minima/maxima\n",
    "- Discuss cost functions and how they work/are used (examples from Linear Regression)\n",
    "- Learn what gradient descent is and what step sizes are\n",
    "- **Activity: Gradient Descent: Step Sizes Lab**\n",
    "- Define what a gradient really is in multiple dimensions\n",
    "- Demonstrate Gradient Descent using a Linear Regression model with Residual Sum of Squares\n",
    "- **Activity: Applying Gradient Descent Lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step Sizes Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to  Derivatives\n",
    "\n",
    "\"Instantaneous rate of change\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.177638Z",
     "start_time": "2021-04-30T15:08:10.175050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.185125Z",
     "start_time": "2021-04-30T15:08:10.182086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot change x,y as lines for x=1 to x=2\n",
    "\n",
    "def jog(hours):\n",
    "    '''\n",
    "    Given some amount of time, in hours, how many miles will we run?\n",
    "    Assumes our pace is 6 mph\n",
    "\n",
    "    Input: hours (time in hours)\n",
    "    Output: number of miles\n",
    "    '''\n",
    "    return 6*hours\n",
    "\n",
    "\n",
    "x1 = 1  # Input at 1 hour\n",
    "y1 = jog(x1)  # Output at 1 hour\n",
    "\n",
    "x2 = 2  # Input at 2 hours\n",
    "y2 = jog(x2)  # Output at 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.373349Z",
     "start_time": "2021-04-30T15:08:10.187184Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# And now, plot\n",
    "fig_jog, ax = plt.subplots(figsize=(7.5, 5.5))\n",
    "\n",
    "# Providing inputs to plot between x=0 and x=3.5\n",
    "x = np.linspace(0, 3.5)\n",
    "\n",
    "# Plotting our function, f(x) = 6x, between 0 and 3.5 hours\n",
    "ax.plot(x, jog(x), label=\"distance given # hours\")\n",
    "\n",
    "# Defining keyword arguments for our dashed lines\n",
    "line_kws = dict(linestyle=\"dashed\", color='darkgray')\n",
    "\n",
    "# Creating our dashed lines\n",
    "ax.hlines(y=y1, xmin=0, xmax=x1, **line_kws)\n",
    "ax.vlines(x=x1, ymin=0, ymax=y1, **line_kws)\n",
    "\n",
    "ax.hlines(y=y2, xmin=0, xmax=x2, **line_kws)\n",
    "ax.vlines(x=x2, ymin=0, ymax=y1, **line_kws)\n",
    "\n",
    "# Creating our \"rise\" portion\n",
    "ax.vlines(x=x2, ymin=y1, ymax=y2, color=\"darkorange\",\n",
    "          label=f\"y2 - y1 = {y2} - {y1} = {y2-y1}\")\n",
    "\n",
    "# Creating our \"run\" portion\n",
    "ax.hlines(y=y1, xmin=x1, xmax=x2, color=\"green\",\n",
    "          label=f\"x2 - x1 = {x2} - {x1}  = {x2-x1}\")\n",
    "\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "ax.set_ylabel(\"distance in miles\")\n",
    "ax.set_xlabel(\"number of hours\")\n",
    "plt.title(\"distance over time, given a 6mph pace\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Functions Complicate Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.616806Z",
     "start_time": "2021-04-30T15:08:10.375875Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plotting something a bit more complicated...\n",
    "fig_nonlinear = plt.figure(figsize=(8,6))\n",
    "\n",
    "# Providing inputs to plot between x=-6 and x=10\n",
    "x_values = np.linspace(-6, 10)\n",
    "# Defining our y values with list comprehension \n",
    "function_values = [2*x**2-8*x for x in x_values]\n",
    "\n",
    "# Plotting our axes at x=0 and y=0\n",
    "plt.axhline(y=0, color='lightgrey', )\n",
    "plt.axvline(x=0, color='lightgrey')\n",
    "\n",
    "# The plot!\n",
    "plt.plot(x_values, function_values, label = \"f(x) = 2x^2 - 8x\")\n",
    "\n",
    "deriv_values = [4*x-8 for x in x_values]\n",
    "plt.plot(x_values, deriv_values, label = \"f'(x)\", color=\"darkorange\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# fig_nonlinear = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The solution is to to decrease $\\Delta x$ to nearly 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large f'(x) = \\displaystyle {\\lim_{ \\Delta x \\to 0}} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If f(x) is:\n",
    "$$f(x) = x^r $$\n",
    "\n",
    "Then, the derivative is: \n",
    "$$ f'(x) = r*x^{r-1} $$\n",
    "\n",
    "\n",
    "- Move the exponent (r) to in front of `x` where it becomes a coefficient of x\n",
    "- Replace the original exponent `r` with `r-1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Factor Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If a variable is multiplied by a constant (i.e. a number), then to take the derivative of that term, apply power rule and multiply the variable by that same constant.\n",
    "\n",
    "So given the function: \n",
    "\n",
    "$$f(x) = 2x^2 $$\n",
    "\n",
    "\n",
    "$$f'(x) = 4x^1 = 4x $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition Terms & Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To take a derivative of a function that has multiple terms, simply take the derivative of each of the terms individually.  So for the function above, \n",
    "\n",
    "$$ f(x) = 4x^3 - x^2 + 3x $$\n",
    "\n",
    "$$ f'(x) = 12x^2 - 2x + 3  $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = 12x + 3$$  \n",
    "$$ f'(x) = 12 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What's the derivative for this function, given that the function is $f(x) = 6x$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.764170Z",
     "start_time": "2021-04-30T15:08:10.619473Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_jog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A: 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What's the derivative for this function, given that the function is $f(x) = 2x^2 - 8x$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.920221Z",
     "start_time": "2021-04-30T15:08:10.766628Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_nonlinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A: $f'(x) = 4x^1 - 8x^0 = 4x-8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Derivatives to Find the Minimum of Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.925009Z",
     "start_time": "2021-04-30T15:08:10.922281Z"
    }
   },
   "outputs": [],
   "source": [
    "from derivatives import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Representing $f(x) = 2x^2 - 8x$ with matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:10.934573Z",
     "start_time": "2021-04-30T15:08:10.928549Z"
    }
   },
   "outputs": [],
   "source": [
    "## [coeff, exp]\n",
    "tuple_sq_pos  = np.array([[2, 2], [-8, 1]])\n",
    "\n",
    "x_values = np.linspace(-6, 10, 100)\n",
    "\n",
    "## calculate output and derivatives\n",
    "function_values = [output_at(tuple_sq_pos, x) for x in  x_values]\n",
    "derivative_values = [derivative_at(tuple_sq_pos, x) for x in x_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:11.414724Z",
     "start_time": "2021-04-30T15:08:10.938176Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,5),ncols=2)\n",
    "\n",
    "# plot 1\n",
    "ax = axes[0]\n",
    "ax.axhline(y=0, color='lightgrey', )\n",
    "ax.axvline(x=0, color='lightgrey')\n",
    "ax.plot(x_values, function_values, label = \"f (x) = 2x^2−8x \")\n",
    "ax.set_title('f(x)')\n",
    "\n",
    "ax.legend(loc=\"upper left\", \n",
    "           bbox_to_anchor=[0, 1], \n",
    "           ncol=2, fancybox=True)\n",
    "\n",
    "# plot 2\n",
    "ax = axes[1]\n",
    "ax.axhline(y=0, color='lightgrey')\n",
    "ax.axvline(x=0, color='lightgrey')\n",
    "ax.plot(x_values, derivative_values,color=\"darkorange\", label = \"f '(x) = 4x-8\")\n",
    "ax.set_title(\"f'(x)\")\n",
    "\n",
    "[ax.grid(True, which='both',ls=':') for ax in axes]\n",
    "ax.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:11.758212Z",
     "start_time": "2021-04-30T15:08:11.416529Z"
    }
   },
   "outputs": [],
   "source": [
    "## making series for convience\n",
    "func_values_series = pd.Series(function_values,index=x_values)#.min()\n",
    "deriv_values_series = pd.Series(derivative_values,index=x_values)\n",
    "\n",
    "(ax1,ax2) = axes\n",
    "ax1.axvline(func_values_series.idxmin(),\n",
    "            ls=':',c='red',label='Function Minimum')\n",
    "ax2.axvline(func_values_series.idxmin(),\n",
    "            ls=':',c='red',label='Function Minimum')\n",
    "[a.legend() for a in [ax1,ax2]]\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The minimum of a function has an instanteous rate of change = 0\n",
    "    - Therefore the value of x that produces a derivative=0 is the minimum of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **So how can we use this fact in Machine Learning?**\n",
    "    - We can use the derivatives of our cost functions for our models to determine what parameters would be best for your model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  This is fundamental concept behind gradient descent (which will discuss in-depth next study group), using derivatives with cost/loss functions to find the best values with the lowest error.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-step-sizes-online-ds-ft-100719/master/images/snowboard.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - A cost function is a function that calculates the error of our models predictions vs ground truth.\n",
    "    - \"Cost function\" = \"Loss function\" = \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multiple notations/names:**\n",
    "\n",
    "    - Loss Function: $L(y,t)$\n",
    "\n",
    "    - Cost Function: $C(y,t)$\n",
    "\n",
    "    - Error: $E(y,t)$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions You've Already Seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error terms we used for linear regression back in Module 1 included Mean Squared Error and Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Sum of Squares\n",
    "\n",
    "$ \\large RSS = \\sum_{i=1}^n(actual - expected)^2 = \\sum_{i=1}^n(y_i - \\hat{y})^2 $\n",
    "\n",
    "\n",
    "#### Mean Squared Error\n",
    "\n",
    "$ \\large MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n",
    "\n",
    "- Note that MSE is just RSS divided by the number of data points. So its the *mean* of the residual sum of squares (AKA squared error)\n",
    "\n",
    "#### Root Mean Squared Error\n",
    "$ \\large  RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\n",
    "- Note that RMSE is just the square root of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Gradient Descent Using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T18:28:02.572470Z",
     "start_time": "2020-03-11T18:28:02.570338Z"
    }
   },
   "source": [
    "## Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:18:48.154660Z",
     "start_time": "2021-04-30T15:18:48.062015Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e4ac03f215b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Kaggle Dataset https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/datasets/house-prices-advanced-regression-techniques/train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## Kaggle Dataset https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "url = \"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/datasets/house-prices-advanced-regression-techniques/train.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:12.277921Z",
     "start_time": "2021-04-30T15:08:12.273794Z"
    }
   },
   "outputs": [],
   "source": [
    "## convert target to $1000 of dollars\n",
    "df['SalePrice ($K)']=df['SalePrice']/1000\n",
    "\n",
    "X = df['GrLivArea'].copy()\n",
    "y = df['SalePrice ($K)'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:12.454215Z",
     "start_time": "2021-04-30T15:08:12.280157Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(X, y, s=3,\n",
    "          alpha=0.7, label=\"raw data\")\n",
    "\n",
    "ax.set(title=f\"{X.name} vs {y.name}\",\n",
    "       ylabel=y.name,\n",
    "       xlabel=X.name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**in Linear Regression, we predict $y$ using 2 parameters, m (slope) + b(intercept/constant):**\n",
    "\n",
    "$$ \\large y = mx+b $$\n",
    "where: \n",
    "    \n",
    "- $x$ = input data for modeling\n",
    "- $y$ = model predictions\n",
    "- $m$ = slope\n",
    "- $b$ = intercept\n",
    "\n",
    "\n",
    "- Since our regression equation is our model, $y$ is really our model's prediction, which we represent as $\\hat{y}$\n",
    "\n",
    "$$ \\large \\hat{y} = mx+b $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:12.635147Z",
     "start_time": "2021-04-30T15:08:12.457119Z"
    }
   },
   "outputs": [],
   "source": [
    "def scatter_plot(X,y):\n",
    "    \"\"\"Plots 2 pandas Series.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    ax.scatter(X, y, s=3,\n",
    "              alpha=0.7, label=\"raw data\")\n",
    "\n",
    "    ax.set(title=f\"{X.name} vs {y.name}\",\n",
    "           ylabel=y.name,\n",
    "           xlabel=X.name);\n",
    "    ax.grid()\n",
    "    return fig,ax\n",
    "fig,ax = scatter_plot(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Let's Attempt to Guess Our Best-Fit Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: Pick an initial starting value for the slope and intercept (take a guess as to what they may be from examining the figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:12.640965Z",
     "start_time": "2021-04-30T15:08:12.637487Z"
    }
   },
   "outputs": [],
   "source": [
    "def regression_model(x,m,b):\n",
    "    return m*x + b\n",
    "\n",
    "def regression_formula(m,b):    \n",
    "    formula = f\"y = {round(m,2)}*x + {round(b,2)}\"\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:12.646188Z",
     "start_time": "2021-04-30T15:08:12.643541Z"
    }
   },
   "outputs": [],
   "source": [
    "## guess slope:\n",
    "slope = 0 \n",
    "## guess intercept:\n",
    "intercept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:12.887816Z",
     "start_time": "2021-04-30T15:08:12.648423Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get Predictions from model\n",
    "y_pred = regression_model(X,slope,intercept)\n",
    "\n",
    "## Plot Raw Data\n",
    "fig,ax = scatter_plot(X,y)\n",
    "\n",
    "## Plot Predicted Values\n",
    "ax.plot(X,y_pred,c='red',ls=':', label = regression_formula(slope,intercept))\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Calculate our Models' Erorr/Cost using RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:22.897741Z",
     "start_time": "2021-04-30T15:08:22.892898Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import r2_score#,mean_squared_error\n",
    "\n",
    "def errors(x_values, y_values, m, b):\n",
    "    \"\"\"(y_i - y_hat_i)\"\"\"\n",
    "    y_line = regression_model(x_values,m,b)#(b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    \"\"\"(y_i - y_hat_i)**2\"\"\"\n",
    "    err_squared = errors(x_values, y_values, m, b)**2\n",
    "    return np.round(err_squared, 2)\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    \"\"\"sum((y_i - y_hat_i)**2)\"\"\"\n",
    "    rss =sum(squared_errors(x_values, y_values, m, b))\n",
    "    return round(rss, 2)\n",
    "\n",
    "def root_mean_squared_error(x_values, y_values, m, b):\n",
    "    rmse = math.sqrt(sum(squared_errors(x_values, y_values, m, b)))/len(x_values)\n",
    "    return round(rmse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:23.667082Z",
     "start_time": "2021-04-30T15:08:23.658344Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a dictionary to collect results\n",
    "res = {}\n",
    "\n",
    "## Calculate RSS\n",
    "res['RSS'] = round(residual_sum_squares(X,y,slope,intercept),2)\n",
    "\n",
    "## Calculate RMSE\n",
    "res['RMSE'] = round(root_mean_squared_error(X,y,slope,intercept),2)\n",
    "\n",
    "## Calculate R2\n",
    "y_pred = regression_model(X,slope,intercept)\n",
    "res['R2'] = round(r2_score(y,y_pred),2)\n",
    "\n",
    "## Print Results\n",
    "[print(f\"{k} = {v}\") for k,v in res.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:24.722906Z",
     "start_time": "2021-04-30T15:08:24.537885Z"
    }
   },
   "outputs": [],
   "source": [
    "## Combine into function\n",
    "def evaluate_model(X,y,slope,intercept, show=False, plot=False):\n",
    "    ## Make a dictionary to collect results\n",
    "    res = {}\n",
    "\n",
    "    ## Calculate RSS\n",
    "    res['RSS'] = round(residual_sum_squares(X,y,slope,intercept),2)\n",
    "    ## Calculate RMSE\n",
    "    res['RMSE'] = round(root_mean_squared_error(X,y,slope,intercept),2)\n",
    "\n",
    "    ## Calculate R2\n",
    "    y_pred = regression_model(X,slope,intercept)\n",
    "    res['R2'] = round(r2_score(y,y_pred),2)\n",
    "\n",
    "    ## Add optional print statement\n",
    "    if show:\n",
    "        [print(f\"{k} = {v}\") for k,v in res.items()]\n",
    "    \n",
    "    ## Add Optional Plot using plotting code from above \n",
    "    if plot:\n",
    "        ## Plot Raw Data\n",
    "        fig,ax = scatter_plot(X,y)\n",
    "\n",
    "        ## Plot Predicted Values\n",
    "        ax.plot(X,y_pred,c='red',ls=':', label = regression_formula(slope,intercept))\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "        \n",
    "    ## Return the res dict and y_preds\n",
    "    return res,y_pred\n",
    "\n",
    "## Use our function below\n",
    "res, _ = evaluate_model(X,y,slope,intercept,True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving towards gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this will be our technique for finding our \"best fit\" line:\n",
    "\n",
    "> * Choose a regression line with a guess of values for $m$ and $b$\n",
    "> * Calculate the RSS\n",
    "> * Adjust $m$ and $b$, as these are the only things that can vary in a single-variable regression line.\n",
    "> * Again calculate the RSS \n",
    "> * Repeat this process\n",
    "> * The regression line (that is, the values of $b$ and $m$) with the smallest RSS is our **best fit line**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Loss Function to Find the Optimal Value of $m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's set a fixed value for our intercept and then lets try a list of possible $m$ values and plot the cost curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:27.182124Z",
     "start_time": "2021-04-30T15:08:27.177932Z"
    }
   },
   "outputs": [],
   "source": [
    "INTERCEPT = 0\n",
    "delta_M = .01\n",
    "\n",
    "## Generate a list of all slopes to try\n",
    "slope_list = np.arange(-2, 2,delta_M)\n",
    "slope_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:28.195788Z",
     "start_time": "2021-04-30T15:08:27.525137Z"
    }
   },
   "outputs": [],
   "source": [
    "## Set up a list to append the m, RSS, and R2 results for each slope\n",
    "res=[['m','RSS','R2']]\n",
    "\n",
    "## For loop trying all possible slopes\n",
    "for m in slope_list:\n",
    "    results, _ = evaluate_model(X,y,m,INTERCEPT)\n",
    "    res.append([m,results['RSS'],results['R2']])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:28.208255Z",
     "start_time": "2021-04-30T15:08:28.198021Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(res[1:], columns=res[0])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:28.706812Z",
     "start_time": "2021-04-30T15:08:28.447451Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = res_df.plot('m','RSS',figsize=(6,4))\n",
    "ax.set(ylabel='RSS',xlabel='Slope Value',title='Cost Curve for m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:28.881295Z",
     "start_time": "2021-04-30T15:08:28.873022Z"
    }
   },
   "outputs": [],
   "source": [
    "res_df.sort_values('RSS').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:29.749435Z",
     "start_time": "2021-04-30T15:08:29.747197Z"
    }
   },
   "outputs": [],
   "source": [
    "best_m = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T15:08:30.471318Z",
     "start_time": "2021-04-30T15:08:30.284477Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use Best Slope to get final model result using evaluate_model\n",
    "evaluate_model(X,y,best_m,INTERCEPT,True,True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The bottom of the blue curve displays the $m$ value that produces the lowest RSS.\n",
    "\n",
    "> - ***So why can't we try every possible option all the time? Why can't we just test out every possible value and find the value that minimizes our loss function?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ***Because our equations will get more complex and  we will be dealing with more than 1 variable to optimize***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent to Find Optimal Parameters - Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The slope of the cost curve tells us our step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The sign (+/-) of the slope indicates if we are approaching the minimum**\n",
    "    * If the slope tilts downwards, then we should walk forward to approach the minimum.  \n",
    "    * And if the slope tilts upwards, then we should point walk backwards to approach the minimum.  \n",
    "* **The steeper the tilt, the further away we are from our cost curve's minimum, so we should take a larger step.**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-step-sizes-online-ds-ft-100719/master/images/snowboard.png\" width=60%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We **cannot** simply use the derivative to find the minimum.  Using that approach will be impossible in many scenarios as our regression lines become more complicated.\n",
    "\n",
    "* We **cannot** alter all of the variables of our regression line across all points and calculate the result.  It will take too much time, as we have more variables to alter. \n",
    "\n",
    "\n",
    "> Let's call each of these changes a **step**, and the size of the change our **step size**. \n",
    "\n",
    "Our new task is to find step sizes that bring us to the best RSS quickly without overshooting the mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True Gradient Descent doesn't just try a fixed number of evenly spaced values.\n",
    "- It uses the size of the slope to indicated **how much** the parameter should change (the **step size**).\n",
    "- We use a parameter called the **learning rate** to control how rapidly we update the parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating our slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **We use the following procedure to find the ideal $m$:**\n",
    "\n",
    "$$\\large m_i = m_{i-1} - \\text{LR}* Loss_{m_i}$$\n",
    "\n",
    "- $m_i$ = Updated slope value for the model. \n",
    "- $m_{i-1}$ = the model's prior slope value\n",
    "\n",
    "- LR represents the Learning Rate\n",
    "    - A decimal between 0 and 1 (usually)\n",
    "- $Cost_{m_i}$ = current slope value from cost function's derivative.\n",
    "\n",
    "> Lesson's Version: <br>\"Update $m$ with the formula $ m = (-.02) * slope_{m = i} + m_i$.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:06:20.911894Z",
     "start_time": "2020-03-16T22:06:20.909740Z"
    }
   },
   "source": [
    "## Activity: Gradient Descent Step Sizes Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - In Topic 23 > labs folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A small learning rate requires many updates before reaching the minimum \n",
    "2. The optimal learning rate quickly converges to the minimum point \n",
    "3. A learning rate that is too large leads to divergent behavior: you may bounce around the minimum!  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-lp-gradient-descent/master/images/learning_rates.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with RSS as a Multivariate Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost Function $J$ (RSS):\n",
    "\n",
    "$$ \\large RSS =  \\sum_{i=1}^n(y_i - \\hat{y})^2 $$ \n",
    "\n",
    "$$ \\large J(m, b) = \\sum_{i=1}^{n}(y_i - \\hat{y})^2 $$\n",
    "\n",
    "- we know $\\hat{y} = mx + b$, so: \n",
    "\n",
    "$$ J(m, b) = \\sum_{i=1}^{n}(y_i - (mx_i + b))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine our cost curve once it includes both $m$ and $b$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-step-sizes-online-ds-pt-100719/master/images/new_gradientdescent.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent in 3D will involve tuning multiple parameters simultaneously.\n",
    "\n",
    "- A 3D Gradient ($\\nabla$) uses **partial derivatives** test the best direction and magnitude to adjust both $m$ and $b$ .\n",
    "\n",
    "$$ \\large \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-step-sizes-online-ds-pt-100719/master/images/new_gradientdescent.png\" width=40%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large f(x, y) = y*x^2 $$\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-students/dsc-gradient-descent-in-3d-online-ds-pt-100719/master/images/new_parabolayx2.png' width=40%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To measure the slope in each dimension, one after the other, we'll take the derivative with respect to one variable, and then take the derivative with respect to another variable.  \n",
    "\n",
    "- To take a derivative with respect to $x$ means to ask, how does the output change, as we make a nudge only in the $x$ direction. \n",
    "- To express that we are nudging in the $x$ direction we say $\\frac{\\delta f}{\\delta x}$. \n",
    "    - We fill in a constant for the value of y (the current value) and then take the derivative with respect to x. \n",
    "\n",
    "-  To express we are nudging in the $y$ direction, we say $\\frac{\\delta f}{\\delta y}$. \n",
    "    - We fill in a constant for the value of x and then take the derivative with respect to y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---#### Gradient Descent - Standing on a Rock\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-in-3d-online-ds-ft-100719/master/images/traveller-stepping.jpg\" width=100%>\n",
    "\n",
    "> So how does this approach of shifting back and forth translate mathematically?  It means we determine the slope in one dimension, then the other. Then, we move where that slope is steepest downwards.  This moves us towards our minimum.\n",
    "\n",
    "--->\n",
    "\n",
    "#### Gradient Descent Upside Down: Mountain Climbing\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-the-gradient-in-gradient-descent-online-ds-ft-100719/master/images/Denali.jpg\">\n",
    "\n",
    "\n",
    "> Here, in finding gradient ascent, our task is not to calculate the gain from a move in either the $x$ or $y$ direction.  Instead, our task is to **find some combination of a change in $x$,$y$ that brings the largest change in output**.  \n",
    "\n",
    "\n",
    "- Our function $f(x,y)$ represents the movement of the climbers towards the summit.\n",
    "\n",
    "- So $\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} $.  \n",
    "\n",
    "<!---\n",
    "- This means that to take the path of greatest ascent, you should move $ \\frac{\\delta f}{\\delta y} $ divided by $ \\frac{\\delta f}{\\delta x} $.  \n",
    "\n",
    "\n",
    "- So for example, when $ \\frac{\\delta f}{\\delta y}f(x, y)  = 3 $ , and $ \\frac{\\delta f}{\\delta x}f(x, y)  = 2$, you traveled in line with a slope of 3/2.\n",
    "\n",
    "- For gradient descent, that is to find the direction of greatest decrease, you simply reverse the direction of your partial derivatives and move in $ - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}$. --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for Calculating RSS Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-gradient-descent-step-sizes-online-ds-pt-100719/master/images/new_gradientdescent.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large J(m, b)= \\sum_{i=1}^n(y_i - (mx_i + b))^2 $$\n",
    "\n",
    "As we know, the gradient of a function is simply the partial derivatives with respect to each of the variables, so:\n",
    "\n",
    "$$ \\large \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta m}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ m}\\\\\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta b}}(y - (mx + b))^2  &&\\text{partial derivative with respect to} \\textbf{ b}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Rule:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The equations for the partial derivatives of our RSS function with respect to m and b are:**\n",
    "\n",
    "\n",
    "$$ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x_i(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$$\n",
    "\n",
    "$$  \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "\n",
    "- Our update rules for m and b become:\n",
    "    - `current_m` = `old_m` $ -  (-2*\\sum_{i=1}^n x_i*\\epsilon_i )$\n",
    "\n",
    "    - `current_b` =  `old_b` $ - ( -2*\\sum_{i=1}^n \\epsilon_i )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Source of these Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **These formulas are derived in one of the Appendix lessons for this section that applies the Chain Rule (also an appendix lesson)**\n",
    "    - Appendix Lesson: The Chain Rule(https://learning.flatironschool.com/courses/2682/assignments/96080?module_item_id=195973)\n",
    "    - Appendix Lesson: Gradient to Cost Function(https://learning.flatironschool.com/courses/2682/pages/gradient-to-cost-function-appendix?module_item_id=195974)\n",
    "    \n",
    "> The chain rule says that we can represent complex equations as multiple equations, and then we can take the derivative of outer and inner equations separately.\n",
    "\n",
    "> $$ \\large F(x) = f(g(x)) $$\n",
    "> $$ \\large F'(x) = f'(g(x))*g'(x) $$\n",
    "\n",
    "\n",
    "\n",
    "- **Applied to our Cost Function:**\n",
    "\n",
    "\n",
    "> \"In calculating the partial derivatives of our function $J(m, b) = \\sum_{i=1}^{n}(y_i - (mx_i + b))^2$, **we won't change the result if we ignore the summation until the very end**.\" \n",
    "\n",
    "$$ J(m, b)= \\sum_{i=1}^n(y_i - (mx_i + b))^2 $$\n",
    "\n",
    "$$ g(m,b) = y - (mx + b)$$ \n",
    "\n",
    "$$ f(g(m,b))= (g(m,b))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply Gradient Descent Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic 23 Folder > labs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Today we:\n",
    "    - Discussed how we can use derivatives to find the best parameter to minimize a cost function. \n",
    "    - Discussed how to extend this to more than one parameter using Partial Derivatives and gradient descent.\n",
    "    - We walked through how we would calculate the $\\beta$s for a Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<!---### The Chain Rule\n",
    "\n",
    "$$h'(x) = (f\\circ g)'(x) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "$$\\frac{dh}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx} = f'(y)g'(x) \\text{  where } y = g(x)$$ --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Practicing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Q: Find the derivative of each function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $ \\large f(x) = 5x^6$\n",
    "    - $f'(x) = ?$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $\\large g(x) = 30x$\n",
    "    - $g'(x)=?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $\\large z(x) = 42 $\n",
    "    - $z'(x) = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T18:02:31.443440Z",
     "start_time": "2020-03-11T18:02:31.440038Z"
    },
    "hidden": true
   },
   "source": [
    "<!--- - $ \\large  F(x) = (3x^3 +20x)^2+5 $ --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $ f(x) = 5x^6$\n",
    "    - $\\large f'(x) = 30x^5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $ g(x) = 30x$\n",
    "    - $\\large g'(x)= 30$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- $ z(x) = 42 $\n",
    "- $ z(x) = 42 * (x^0) $\n",
    "- $ \\large z'(x) = 0*42x^{0-1} = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- For a linear function:\n",
    "\n",
    "$$\n",
    "f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\large f(x, y) = y*x^2 $$\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-students/dsc-gradient-descent-in-3d-online-ds-pt-100719/master/images/new_parabolayx2.png' width=40%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- To measure the slope in each dimension, one after the other, we'll take the derivative with respect to one variable, and then take the derivative with respect to another variable.  \n",
    "\n",
    "- To take a derivative with respect to $x$ means to ask, how does the output change, as we make a nudge only in the $x$ direction. \n",
    "- To express that we are nudging in the $x$ direction we say $\\frac{\\delta f}{\\delta x}$. \n",
    "    - We fill in a constant for the value of y (the current value) and then take the derivative with respect to x. \n",
    "\n",
    "-  To express we are nudging in the $y$ direction, we say $\\frac{\\delta f}{\\delta y}$. \n",
    "    - We fill in a constant for the value of x and then take the derivative with respect to y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So what does a derivative $\\frac{\\delta f}{\\delta x}$ look like? How do we think of a partial derivative of a multivariable function?\n",
    "\n",
    "Well, remember how we think of a standard derivative of a one variable function, for example $f(x) = x^2 $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\frac{df}{dy}f(x, y)$ where $f(x,y) = (yx^2) $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Gradient Descent Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Generalized Gradient Descent  Math Notation\n",
    "\n",
    "- In the below equations: \n",
    "     - x and x_i represent whatever paramater is being tested and updated. \n",
    "     \n",
    " \n",
    "- Multi-dimensional version of a derivative \n",
    "$$-\\nabla = \\sum_i \\dfrac{\\partial}{\\partial x_i}$$\n",
    "\n",
    "$$ x_{i+1} = x_i - \\eta * f'(x_i)$$\n",
    "\n",
    "\n",
    "- When minimum is reached, $ \\eta *f'(x_i) $ becomes 0. \n",
    "$$ x_{i+1} = x_i - 0 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Walkthrough_Gradient_Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "453.6px",
    "left": "0px",
    "top": "110.833px",
    "width": "230.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
